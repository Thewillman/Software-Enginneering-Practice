import jieba
import sys
from gensim import corpora, models, similarities
from jieba import analyse
import time
import logging


class TextSameError(Exception):  # æ–‡æœ¬ä¸åŒç›¸ä¼¼åº¦å´100%

    def __init__(self):
        print("æ–‡æœ¬ä¸ä¸€æ ·ä¸ºä»€ä¹ˆä¼šå®Œå…¨ä¸€æ ·æï¼Ÿ")

    def __str__(self, *args, **kwargs):
        return "å†æ£€æŸ¥ä¸€ä¸‹ä»£ç å“¦"


class TextDifferentError(Exception):  # æ–‡æœ¬ä¸€è‡´ç›¸ä¼¼åº¦ä¸æ˜¯100%

    def __init__(self):
        print("è‡ªå·±å’Œè‡ªå·±æ¯”ï¼Œæ€ä¹ˆä¼šä¸ä¸€æ ·å‘¢ï¼Ÿ")

    def __str__(self, *args, **kwargs):
        return "å†æ£€æŸ¥ä¸€ä¸‹ä»£ç å“¦"


class NoChineseError(Exception):  # æ¯”å¯¹æ–‡æœ¬å‹æ ¹æ²¡æœ‰æ±‰å­—ï¼Œç›¸ä¼¼åº¦ç›´æ¥åˆ¤0


    def __init__(self):
        print("æ±‰å­—éƒ½æ²¡æœ‰æ¯”å¯¹ğŸ”¨å‘¢")
        ans_txt = open(sys.argv[3], 'w', encoding='UTF-8')
        sim = str(0.00)
        ans_txt.write(sim)
        ans_txt.close()
        print("0")

    def __str__(self, *args, **kwargs):
        return "æ‰¾ç¯‡æœ‰æ±‰å­—çš„æ¥å§"

#@profile
def create_jieba_list(test_data):  # å°†æ–‡æœ¬åˆ†å¥å¹¶ä¸”æ¯ä¸ªå¥å­è¿›è¡Œåˆ†è¯
    test_sentence = []
    s = ""
    for i in range(0, len(test_data)):
        if '\u4e00' <= test_data[i] <= '\u9fff':  # åªè®°å½•æ±‰å­—
            s += test_data[i]
        elif test_data[i] == 'ã€‚':  # ä»¥å¥å·ä½œä¸ºåˆ†å¥æ ‡å‡†
            # print(s)
            if s != "":  # é˜²æ­¢å‡ºç°ä¸€è¿ä¸²ç¬¦å·
                test_sentence.append(s)
                s = ""
    if s != "":  # å¯èƒ½è¿˜æœ‰æ–‡æœ¬éœ€è¦åŠ ä¸Š
        # print(s)
        test_sentence.append(s)
        s = ""
    test_items = [[i for i in jieba.lcut(item)] for item in test_sentence]
    return test_items

#@profile
def cal_sentence_weight(test_data):  # arrayæ•°ç»„å­˜æŠ„è¢­æ–‡æœ¬æ¯å¥å æ€»æ–‡æœ¬æ±‰å­—çš„æƒé‡ï¼Œcntæ˜¯æ¯ä¸ªå¥å­æ±‰å­—é•¿åº¦ï¼Œsumæ˜¯æ€»æ–‡æœ¬æ±‰å­—é•¿åº¦
    array = []
    cnt = 0
    sum = 0
    # ç»Ÿè®¡å¥å­æ±‰å­—é•¿åº¦ä»¥åŠæ€»æ–‡æœ¬æ±‰å­—é•¿åº¦
    for i in range(0, len(test_data)):
        if '\u4e00' <= test_data[i] <= '\u9fff':
            cnt += 1
        elif test_data[i] == 'ã€‚':
            if cnt:
                array.append(cnt)
                sum += cnt
                cnt = 0
    if cnt != 0:
        # print(s)
        array.append(cnt)
        cnt = 0
    # è®¡ç®—æƒé‡
    if sum == 0:
        raise NoChineseError
    else:
        for i in range(0, len(array)):
            array[i] = array[i] * 1.0 / sum
        return array

#@profile
def cal_similarity_tfidf(orig_items, orig_sim_items, array):
    # å¯¹åŸå§‹æ–‡æœ¬ç”¨gensimåº“ä¸­çš„doc2bowå’Œcorporaè¿›è¡Œå¤„ç†ï¼Œé‡‡ç”¨tfidfæ¨¡å‹
    # ç”Ÿæˆè¯å…¸
    dictionary = corpora.Dictionary(orig_items)
    # é€šè¿‡doc2bowç¨€ç–å‘é‡ç”Ÿæˆè¯­æ–™åº“
    corpus = [dictionary.doc2bow(item) for item in orig_items]
    # é€šè¿‡TFæ¨¡å‹ç®—æ³•ï¼Œè®¡ç®—å‡ºtfå€¼
    tf = models.TfidfModel(corpus)
    # é€šè¿‡token2idå¾—åˆ°ç‰¹å¾æ•°ï¼ˆå­—å…¸é‡Œé¢çš„é”®çš„ä¸ªæ•°ï¼‰
    num_features = len(dictionary.token2id.keys())
    # è®¡ç®—ç¨€ç–çŸ©é˜µç›¸ä¼¼åº¦ï¼Œå»ºç«‹ä¸€ä¸ªç´¢å¼•
    index = similarities.MatrixSimilarity(tf[corpus], num_features=num_features)

    # å¼€å§‹å¯¹æŠ„è¢­æ–‡æœ¬çš„ç›¸ä¼¼åº¦è¿›è¡Œè®¡ç®—
    ans = 0.0
    for i in range(0, len(orig_sim_items)):
        # æŠŠæ¯ä¸ªåˆ†å¥½è¯çš„å¥å­å»ºç«‹æˆæ–°çš„ç¨€ç–å‘é‡å¹¶ä»£å…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        orig_sim_vec = dictionary.doc2bow(orig_sim_items[i])
        sim = index[tf[orig_sim_vec]]
        sim_max = max(sim)
        if sim_max < 0.0025:  # å¯¹äºç›¸ä¼¼åº¦ä½äº0.25%çš„å¥å­æˆ‘ä»¬ç›´æ¥è§†ä¸ºä¸ç›¸å…³
            continue
        try:
            ans += max(sim) * array[i]  # æ˜¾ç„¶æˆ‘ä»¬è¿™é‡Œè¦å–æœ€é«˜ç›¸ä¼¼åº¦è€Œä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œå¯èƒ½ä¼šæœ‰ä¸‹æ ‡è¶…ç•Œçš„é”™è¯¯
        except IndexError:
            print("orzä¸‹æ ‡è¶…ç•Œäº†")
        else:
            continue
    return ans

def cal_similarity_lsi(orig_items, orig_sim_items, array):
    # å¯¹åŸå§‹æ–‡æœ¬ç”¨gensimåº“ä¸­çš„doc2bowå’Œcorporaè¿›è¡Œå¤„ç†ï¼Œé‡‡ç”¨tfidfæ¨¡å‹
    # ç”Ÿæˆè¯å…¸
    dictionary = corpora.Dictionary(orig_items)
    # é€šè¿‡doc2bowç¨€ç–å‘é‡ç”Ÿæˆè¯­æ–™åº“
    corpus = [dictionary.doc2bow(item) for item in orig_items]
    # é€šè¿‡TFæ¨¡å‹ç®—æ³•ï¼Œè®¡ç®—å‡ºtfå€¼
    lsi = models.LsiModel(corpus)
    # é€šè¿‡token2idå¾—åˆ°ç‰¹å¾æ•°ï¼ˆå­—å…¸é‡Œé¢çš„é”®çš„ä¸ªæ•°ï¼‰
    num_features = len(dictionary.token2id.keys())
    # è®¡ç®—ç¨€ç–çŸ©é˜µç›¸ä¼¼åº¦ï¼Œå»ºç«‹ä¸€ä¸ªç´¢å¼•
    index = similarities.MatrixSimilarity(lsi[corpus], num_features=num_features)

    # å¼€å§‹å¯¹æŠ„è¢­æ–‡æœ¬çš„ç›¸ä¼¼åº¦è¿›è¡Œè®¡ç®—
    ans = 0.0
    for i in range(0, len(orig_sim_items)):
        # æŠŠæ¯ä¸ªåˆ†å¥½è¯çš„å¥å­å»ºç«‹æˆæ–°çš„ç¨€ç–å‘é‡å¹¶ä»£å…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        orig_sim_vec = dictionary.doc2bow(orig_sim_items[i])
        sim = index[lsi[orig_sim_vec]]
        sim_max = max(sim)
        if sim_max < 0.0025:  # å¯¹äºç›¸ä¼¼åº¦ä½äº0.25%çš„å¥å­æˆ‘ä»¬ç›´æ¥è§†ä¸ºä¸ç›¸å…³
            continue
        try:
            ans += max(sim) * array[i]  # æ˜¾ç„¶æˆ‘ä»¬è¿™é‡Œè¦å–æœ€é«˜ç›¸ä¼¼åº¦è€Œä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œå¯èƒ½ä¼šæœ‰ä¸‹æ ‡è¶…ç•Œçš„é”™è¯¯
        except IndexError:
            print("orzä¸‹æ ‡è¶…ç•Œäº†")
        else:
            continue
    return ans

def cal_similarity_lda(orig_items, orig_sim_items, array):
    # å¯¹åŸå§‹æ–‡æœ¬ç”¨gensimåº“ä¸­çš„doc2bowå’Œcorporaè¿›è¡Œå¤„ç†ï¼Œé‡‡ç”¨tfidfæ¨¡å‹
    # ç”Ÿæˆè¯å…¸
    dictionary = corpora.Dictionary(orig_items)
    # é€šè¿‡doc2bowç¨€ç–å‘é‡ç”Ÿæˆè¯­æ–™åº“
    corpus = [dictionary.doc2bow(item) for item in orig_items]
    # é€šè¿‡TFæ¨¡å‹ç®—æ³•ï¼Œè®¡ç®—å‡ºtfå€¼
    lda = models.LdaModel(corpus)
    # é€šè¿‡token2idå¾—åˆ°ç‰¹å¾æ•°ï¼ˆå­—å…¸é‡Œé¢çš„é”®çš„ä¸ªæ•°ï¼‰
    num_features = len(dictionary.token2id.keys())
    # è®¡ç®—ç¨€ç–çŸ©é˜µç›¸ä¼¼åº¦ï¼Œå»ºç«‹ä¸€ä¸ªç´¢å¼•
    index = similarities.MatrixSimilarity(lda[corpus], num_features=num_features)

    # å¼€å§‹å¯¹æŠ„è¢­æ–‡æœ¬çš„ç›¸ä¼¼åº¦è¿›è¡Œè®¡ç®—
    ans = 0.0
    for i in range(0, len(orig_sim_items)):
        # æŠŠæ¯ä¸ªåˆ†å¥½è¯çš„å¥å­å»ºç«‹æˆæ–°çš„ç¨€ç–å‘é‡å¹¶ä»£å…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        orig_sim_vec = dictionary.doc2bow(orig_sim_items[i])
        sim = index[lda[orig_sim_vec]]
        sim_max = max(sim)
        if sim_max < 0.0025:  # å¯¹äºç›¸ä¼¼åº¦ä½äº0.25%çš„å¥å­æˆ‘ä»¬ç›´æ¥è§†ä¸ºä¸ç›¸å…³
            continue
        try:
            ans += max(sim) * array[i]  # æ˜¾ç„¶æˆ‘ä»¬è¿™é‡Œè¦å–æœ€é«˜ç›¸ä¼¼åº¦è€Œä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œå¯èƒ½ä¼šæœ‰ä¸‹æ ‡è¶…ç•Œçš„é”™è¯¯
        except IndexError:
            print("orzä¸‹æ ‡è¶…ç•Œäº†")
        else:
            continue
    return ans

def cal_similarity_word2vec(orig_items, orig_sim_items, array):
    # å¯¹åŸå§‹æ–‡æœ¬ç”¨gensimåº“ä¸­çš„doc2bowå’Œcorporaè¿›è¡Œå¤„ç†ï¼Œé‡‡ç”¨tfidfæ¨¡å‹
    # ç”Ÿæˆè¯å…¸
    dictionary = corpora.Dictionary(orig_items)
    # é€šè¿‡doc2bowç¨€ç–å‘é‡ç”Ÿæˆè¯­æ–™åº“
    corpus = [dictionary.doc2bow(item) for item in orig_items]
    # é€šè¿‡TFæ¨¡å‹ç®—æ³•ï¼Œè®¡ç®—å‡ºtfå€¼
    word2vector = models.word2vec(corpus)
    # é€šè¿‡token2idå¾—åˆ°ç‰¹å¾æ•°ï¼ˆå­—å…¸é‡Œé¢çš„é”®çš„ä¸ªæ•°ï¼‰
    num_features = len(dictionary.token2id.keys())
    # è®¡ç®—ç¨€ç–çŸ©é˜µç›¸ä¼¼åº¦ï¼Œå»ºç«‹ä¸€ä¸ªç´¢å¼•
    index = similarities.MatrixSimilarity(word2vector[corpus], num_features=num_features)

    # å¼€å§‹å¯¹æŠ„è¢­æ–‡æœ¬çš„ç›¸ä¼¼åº¦è¿›è¡Œè®¡ç®—
    ans = 0.0
    for i in range(0, len(orig_sim_items)):
        # æŠŠæ¯ä¸ªåˆ†å¥½è¯çš„å¥å­å»ºç«‹æˆæ–°çš„ç¨€ç–å‘é‡å¹¶ä»£å…¥æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
        orig_sim_vec = dictionary.doc2bow(orig_sim_items[i])
        sim = index[word2vector[orig_sim_vec]]
        sim_max = max(sim)
        if sim_max < 0.0025:  # å¯¹äºç›¸ä¼¼åº¦ä½äº0.25%çš„å¥å­æˆ‘ä»¬ç›´æ¥è§†ä¸ºä¸ç›¸å…³
            continue
        try:
            ans += max(sim) * array[i]  # æ˜¾ç„¶æˆ‘ä»¬è¿™é‡Œè¦å–æœ€é«˜ç›¸ä¼¼åº¦è€Œä¸æ˜¯ä¸€ä¸€å¯¹åº”ï¼Œå¯èƒ½ä¼šæœ‰ä¸‹æ ‡è¶…ç•Œçš„é”™è¯¯
        except IndexError:
            print("orzä¸‹æ ‡è¶…ç•Œäº†")
        else:
            continue
    return ans
if __name__ == '__main__':
    start = time.time()
    orig = open(sys.argv[1], 'r', encoding='UTF-8')
    orig_text = orig.read()
    orig.close()
    # è½½å…¥åœç”¨è¯
    jieba.analyse.set_stop_words("stopword.txt")
    # 1.å°†åŸå§‹æ–‡æœ¬åˆ†å¥å¹¶æ¯å¥ç”¨jieba_lcutåˆ†è¯
    orig_items = create_jieba_list(orig_text)

    # print(len(array))

    # 7.å¯¹å¾…è¯„æµ‹æ–‡æœ¬è¿›è¡ŒåŒæ ·å¤„ç†
    orig_sim = open(sys.argv[2], 'r', encoding='UTF-8')
    orig_sim_text = orig_sim.read()
    orig_sim.close()
    orig_sim_items = create_jieba_list(orig_sim_text)
    # print(test_sentence)

    array = cal_sentence_weight(orig_sim_text)
    # print(len(test_items))
    ans = cal_similarity_tfidf(orig_items, orig_sim_items, array)
    print('tf-idfæ¨¡å‹ç›¸ä¼¼åº¦: %.2f' %ans)
    ans = cal_similarity_lsi(orig_items, orig_sim_items, array)
    print('lsiæ¨¡å‹ç›¸ä¼¼åº¦:%.2f' %ans)
    ans = cal_similarity_lda(orig_items, orig_sim_items, array)
    print('ldaæ¨¡å‹ç›¸ä¼¼åº¦:%.2f' % ans)
    ans = cal_similarity_hdp(orig_items, orig_sim_items, array)
    print('hdpæ¨¡å‹ç›¸ä¼¼åº¦:%.2f' % ans)
    ans = cal_similarity_word2vec(orig_items, orig_sim_items, array)
    print('word2vecæ¨¡å‹ç›¸ä¼¼åº¦:%.2f' % ans)
    ans = cal_similarity_tfidf(orig_items, orig_sim_items, array)
    if abs(ans - 1.00) <= 0.000001 :
        raise TextSameError
    else:
        ans_txt = open(sys.argv[3], 'w', encoding='UTF-8')
        sim = str('%.2f' % ans)
        ans_txt.write(sim)
        ans_txt.close()
        end = time.time()
    print(0)
